{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ae55614-0b29-4d6f-9d2b-bf3cfee974e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "###library###\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm #for fit-t\n",
    "from scipy.stats import norm #for VaR and ES with normal\n",
    "from scipy.stats import t #for VaR and ES with T\n",
    "from scipy.optimize import minimize #for fit-t and VaR T\n",
    "from scipy.integrate import quad #for ES simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "422e8044-17e7-43d9-b0f6-4a152df44acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "###test 1###\n",
    "def calculate_matrix(df, skipRow=True, method='cov'):\n",
    "    df = pd.DataFrame(df) if not isinstance(df, pd.DataFrame) else df\n",
    "    func = np.cov if method == 'cov' else np.corrcoef\n",
    "    if skipRow:\n",
    "        df = df.dropna(axis=0, how='any')\n",
    "    \n",
    "    if df.isnull().any().any() and not skipRow:\n",
    "        n = df.shape[1]\n",
    "        matrix = np.empty((n, n))\n",
    "        matrix.fill(np.nan)\n",
    "        for i in range(n):\n",
    "            for j in range(i + 1):\n",
    "                valid_rows = df.iloc[:, [i, j]].dropna().index\n",
    "                if len(valid_rows) > 0:\n",
    "                    matrix_ij = func(df.iloc[valid_rows, [i, j]], rowvar=False)[0, 1]\n",
    "                    matrix[i, j] = matrix_ij\n",
    "                    matrix[j, i] = matrix_ij\n",
    "    else:\n",
    "        matrix = func(df.T)\n",
    "    \n",
    "    return matrix\n",
    "\n",
    "# Load the data from CSV files\n",
    "df = pd.read_csv('/Users/queenieliu/FinTech545_Spring2024/testfiles/data/test1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "954c0ab4-002a-412b-8024-10c1b63167b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison result between calculated covariance and expected output1.1:\n",
      " [[ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]]\n"
     ]
    }
   ],
   "source": [
    "#testout_1.1 compare\n",
    "expected_cov1 = pd.read_csv('/Users/queenieliu/FinTech545_Spring2024/testfiles/data/testout_1.1.csv')\n",
    "# Convert expected_cov to a numpy array if it's not already, assuming it's the expected covariance matrix\n",
    "if isinstance(expected_cov1, pd.DataFrame):\n",
    "    expected_cov1 = expected_cov1.values\n",
    "# Calculate covariance matrix skipping rows with missing values\n",
    "result1 = calculate_matrix(df, skipRow=True, method='cov')\n",
    "# Compare `res1` with `expected_cov`\n",
    "comparison_result1 = np.isclose(expected_cov1, result1, atol=1e-5)\n",
    "print(\"Comparison result between calculated covariance and expected output1.1:\\n\", comparison_result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "141c26df-6dd3-4c84-8e65-e3ff522097b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison result between calculated correlation and expected output1.2:\n",
      " [[ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]]\n"
     ]
    }
   ],
   "source": [
    "#testout_1.2 compare\n",
    "expected_corr1 = pd.read_csv('/Users/queenieliu/FinTech545_Spring2024/testfiles/data/testout_1.2.csv').values\n",
    "if isinstance(expected_corr1, pd.DataFrame):\n",
    "    expected_corr1 = expected_corr1.values\n",
    "result2 = calculate_matrix(df, skipRow=True, method='corr')\n",
    "comparison_result2 = np.isclose(expected_corr1, result2, atol=1e-5)\n",
    "print(\"Comparison result between calculated correlation and expected output1.2:\\n\", comparison_result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79fd28f4-f194-43d1-be01-ee50b3e78699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison result between calculated covariance and expected output1.3:\n",
      " [[ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]]\n"
     ]
    }
   ],
   "source": [
    "#testout_1.3 compare\n",
    "expected_cov2 = pd.read_csv('/Users/queenieliu/FinTech545_Spring2024/testfiles/data/testout_1.3.csv').values\n",
    "if isinstance(expected_cov2, pd.DataFrame):\n",
    "    expected_corr2 = expected_cov2.values\n",
    "result3 = calculate_matrix(df, skipRow=False, method='cov')\n",
    "comparison_result3 = np.isclose(expected_cov2, result3, atol=1e-5)\n",
    "print(\"Comparison result between calculated covariance and expected output1.3:\\n\", comparison_result3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "064b5b31-dc8a-47ad-aafb-e6f56c190374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison result between calculated correlation and expected output1.4:\n",
      " [[ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]]\n"
     ]
    }
   ],
   "source": [
    "#testout_1.4 compare\n",
    "expected_corr2 = pd.read_csv('/Users/queenieliu/FinTech545_Spring2024/testfiles/data/testout_1.4.csv').values\n",
    "if isinstance(expected_corr2, pd.DataFrame):\n",
    "    expected_corr2 = expected_corr2.values\n",
    "result4 = calculate_matrix(df, skipRow=False, method='corr')\n",
    "comparison_result4 = np.isclose(expected_corr2, result4, atol=1e-5)\n",
    "print(\"Comparison result between calculated correlation and expected output1.4:\\n\", comparison_result4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c0c0d9a-dad8-4084-a53f-2749eca75c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "###test 2###\n",
    "def expW(m, lam):\n",
    "    w = np.empty(m)\n",
    "    for i in range(m):\n",
    "        w[i] = (1 - lam) * lam ** (m - i - 1)  # Adjusted index to 0-based\n",
    "    # Normalize weights to sum to 1\n",
    "    w /= np.sum(w)\n",
    "    return w\n",
    "\n",
    "def ewMatrix(x, lam, matrix_type):\n",
    "    m, n = x.shape\n",
    "    w = expW(m, lam)\n",
    "    # Remove the mean from each column\n",
    "    xm = np.mean(x, axis=0)\n",
    "    x -= xm\n",
    "    # Reshape w from (m,) to (m, 1) to allow broadcasting\n",
    "    w = w.reshape(-1, 1)\n",
    "    # Calculate exponentially weighted covariance\n",
    "    cov_matrix = np.dot((w * x).T, x)\n",
    "    \n",
    "    if matrix_type == 'cov':\n",
    "        return cov_matrix\n",
    "    elif matrix_type == 'corr':\n",
    "        # Convert covariance to correlation\n",
    "        std_dev = np.sqrt(np.diag(cov_matrix))\n",
    "        corr_matrix = cov_matrix / np.outer(std_dev, std_dev)\n",
    "        return corr_matrix\n",
    "    else:\n",
    "        raise ValueError(\"matrix_type must be either 'cov' or 'corr'\")\n",
    "\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('/Users/queenieliu/FinTech545_Spring2024/testfiles/data/test2.csv')\n",
    "# Ensure the data is in a NumPy array for processing\n",
    "data_matrix = df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "704bb059-9617-42e6-80b4-aecafa65b9a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison result between calculated exponentially weighted covariance and expected output:\n",
      " [[ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]]\n"
     ]
    }
   ],
   "source": [
    "#testout_2.1\n",
    "expected_res1 = pd.read_csv('/Users/queenieliu/FinTech545_Spring2024/testfiles/data/testout_2.1.csv')\n",
    "if isinstance(expected_res1, pd.DataFrame):\n",
    "    expected_res1 = expected_res1.values\n",
    "# Calculate the expoentially weighted covariance matrix\n",
    "result1 = ewMatrix(data_matrix, lam=0.97, matrix_type='cov')\n",
    "# Ensure the shapes of result1 and expected_res1 match before comparison\n",
    "comparison_result1 = np.isclose(expected_res1, result1, atol=1e-5)\n",
    "print(\"Comparison result between calculated exponentially weighted covariance and expected output:\\n\", comparison_result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b297a785-c974-4851-a36d-742d317421d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison result between calculated exponentially weighted Correlation and expected output:\n",
      " [[ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]]\n"
     ]
    }
   ],
   "source": [
    "#testout_2.2\n",
    "expected_res2 = pd.read_csv('/Users/queenieliu/FinTech545_Spring2024/testfiles/data/testout_2.2.csv')\n",
    "if isinstance(expected_res2, pd.DataFrame):\n",
    "    expected_res2 = expected_res2.values\n",
    "# Calculate the expoentially weighted covariance matrix\n",
    "result2 = ewMatrix(data_matrix, lam=0.94, matrix_type='corr')\n",
    "# Ensure the shapes of result2 and expected_res2 match before comparison\n",
    "comparison_result2 = np.isclose(expected_res2, result2, atol=1e-5)\n",
    "print(\"Comparison result between calculated exponentially weighted Correlation and expected output:\\n\", comparison_result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9df9e5ec-97e9-4a4b-a0ef-fe5422df0b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison result is:\n",
      " [[ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]]\n"
     ]
    }
   ],
   "source": [
    "# testout_2.3\n",
    "expected_res3 = pd.read_csv('/Users/queenieliu/FinTech545_Spring2024/testfiles/data/testout_2.3.csv')\n",
    "cov = ewMatrix(data_matrix, 0.97, matrix_type='cov')\n",
    "sd1 = np.sqrt(np.diag(cov))\n",
    "cov = ewMatrix(data_matrix, 0.94, matrix_type='cov')\n",
    "sd = 1 / np.sqrt(np.diag(cov))\n",
    "result3 = np.diag(sd1) @ np.diag(sd) @ cov @ np.diag(sd) @ np.diag(sd1)\n",
    "comparison_result3 = np.isclose(expected_res3, result3, atol=1e-5)\n",
    "print(\"Comparison result is:\\n\", comparison_result3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c27f63e-312c-41c0-a449-5a971a6bb487",
   "metadata": {},
   "outputs": [],
   "source": [
    "###test 3###\n",
    "def near_psd(a, epsilon=0.0):\n",
    "    n = a.shape[0]\n",
    "    out = a.copy()\n",
    "    invSD = None  # Initialize invSD\n",
    "\n",
    "    # Check if the matrix is a covariance matrix and convert it to a correlation matrix if needed\n",
    "    if not np.allclose(np.diag(out), 1):\n",
    "        invSD = np.diag(1.0 / np.sqrt(np.diag(out)))\n",
    "        out = invSD @ out @ invSD\n",
    "\n",
    "    # Perform SVD, update eigenvalues, and scale\n",
    "    vals, vecs = np.linalg.eigh(out)\n",
    "    vals = np.maximum(vals, epsilon)\n",
    "    T = 1.0 / (vecs ** 2 @ vals[:, np.newaxis])\n",
    "    T = np.diag(np.sqrt(T).flatten())\n",
    "    l = np.diag(np.sqrt(vals))\n",
    "    B = T @ vecs @ l\n",
    "    out = B @ B.T\n",
    "\n",
    "    # Add back the variance if needed\n",
    "    if invSD is not None:\n",
    "        invSD = np.diag(1.0 / np.diag(invSD))\n",
    "        out = invSD @ out @ invSD\n",
    "\n",
    "    return out\n",
    "\n",
    "def frobenius(input):\n",
    "    result = 0\n",
    "    for i in range(len(input)):\n",
    "        for j in range(len(input)):\n",
    "            result += input[i][j]**2\n",
    "    return result\n",
    "\n",
    "# define a function calculating PSD via Higham's method\n",
    "def higham_nearestPSD(input):\n",
    "    weight = np.identity(len(input))\n",
    "        \n",
    "    norml = np.inf\n",
    "    Yk = input.copy()\n",
    "    Delta_S = np.zeros_like(Yk)\n",
    "    \n",
    "    invSD = None\n",
    "    if np.count_nonzero(np.diag(Yk) == 1.0) != input.shape[0]:\n",
    "        invSD = np.diag(1 / np.sqrt(np.diag(Yk)))\n",
    "        Yk = invSD @ Yk @ invSD\n",
    "    \n",
    "    Y0 = Yk.copy()\n",
    "\n",
    "    for i in range(1000):\n",
    "        Rk = Yk - Delta_S\n",
    "        Xk = np.sqrt(weight)@ Rk @np.sqrt(weight)\n",
    "        vals, vecs = np.linalg.eigh(Xk)\n",
    "        vals = np.where(vals > 0, vals, 0)\n",
    "        Xk = np.sqrt(weight)@ vecs @ np.diagflat(vals) @ vecs.T @ np.sqrt(weight)\n",
    "        Delta_S = Xk - Rk\n",
    "        Yk = Xk.copy()\n",
    "        np.fill_diagonal(Yk, 1)\n",
    "        norm = frobenius(Yk-Y0)\n",
    "        min_val = np.real(np.linalg.eigvals(Yk)).min()\n",
    "        if abs(norm - norml) < 1e-8 and min_val > -1e-9:\n",
    "            break\n",
    "        else:\n",
    "            norml = norm\n",
    "    \n",
    "    if invSD is not None:\n",
    "        invSD = np.diag(1 / np.diag(invSD))\n",
    "        Yk = invSD @ Yk @ invSD\n",
    "    return Yk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1254853b-5a0f-4dba-9b9b-3dd8cc5ecb49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison result is:\n",
      " [[ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]]\n"
     ]
    }
   ],
   "source": [
    "#test 3.1\n",
    "cov_matrix_1 = pd.read_csv('/Users/queenieliu/FinTech545_Spring2024/testfiles/data/testout_1.3.csv').values\n",
    "cov_matrix_2 = pd.read_csv('/Users/queenieliu/FinTech545_Spring2024/testfiles/data/testout_3.1.csv').values\n",
    "# Calculate the near PSD covariance matrices\n",
    "near_psd_cov_1 = near_psd(cov_matrix_1, epsilon=0.0)\n",
    "near_psd_cov_2 = near_psd(cov_matrix_2, epsilon=0.0)\n",
    "# Compare the results under atol=1e-5\n",
    "comparison_result_3_1 = np.isclose(near_psd_cov_1, near_psd_cov_2, atol=1e-5)\n",
    "print(\"Comparison result is:\\n\", comparison_result_3_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c87fbb60-cf48-4092-8dd7-5030a2f3dff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison result is:\n",
      " [[ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]]\n"
     ]
    }
   ],
   "source": [
    "#test 3.2\n",
    "# Load the correlation matrices from the provided files\n",
    "corr_matrix_1 = pd.read_csv('/Users/queenieliu/FinTech545_Spring2024/testfiles/data/testout_1.4.csv').values\n",
    "corr_matrix_2 = pd.read_csv('/Users/queenieliu/FinTech545_Spring2024/testfiles/data/testout_3.2.csv').values\n",
    "# Calculate the near PSD correlation matrices\n",
    "near_psd_corr_1 = near_psd(corr_matrix_1, epsilon=0.0)\n",
    "near_psd_corr_2 = near_psd(corr_matrix_2, epsilon=0.0)\n",
    "# Compare the results under atol=1e-5\n",
    "comparison_result_3_2 = np.isclose(near_psd_corr_1, near_psd_corr_2, atol=1e-5)\n",
    "print(\"Comparison result is:\\n\", comparison_result_3_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea99e554-7214-4fba-b8ca-2586c7a932b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison result is:\n",
      " [[ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]]\n"
     ]
    }
   ],
   "source": [
    "#test 3.3\n",
    "# Load the correlation matrices from the provided files\n",
    "cov_matrix_1 = pd.read_csv('/Users/queenieliu/FinTech545_Spring2024/testfiles/data/testout_1.3.csv').values\n",
    "cov_matrix_2 = pd.read_csv('/Users/queenieliu/FinTech545_Spring2024/testfiles/data/testout_3.3.csv').values\n",
    "# Calculate the near PSD correlation matrices\n",
    "highamnear_psd_cov1 = higham_nearestPSD(cov_matrix_1)\n",
    "highamnear_psd_cov2 = higham_nearestPSD(cov_matrix_2)\n",
    "# Compare the results under atol=1e-5\n",
    "comparison_result_3_3 = np.isclose(highamnear_psd_cov1, highamnear_psd_cov2, atol=1e-5)\n",
    "print(\"Comparison result is:\\n\", comparison_result_3_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29cc25d9-c13c-4872-9345-ee706cfaf0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison result is:\n",
      " [[ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]]\n"
     ]
    }
   ],
   "source": [
    "#test 3.4\n",
    "# Load the correlation matrices from the provided files\n",
    "corr_matrix_1 = pd.read_csv('/Users/queenieliu/FinTech545_Spring2024/testfiles/data/testout_1.4.csv').values\n",
    "corr_matrix_2 = pd.read_csv('/Users/queenieliu/FinTech545_Spring2024/testfiles/data/testout_3.4.csv').values\n",
    "# Calculate the near PSD correlation matrices\n",
    "highamnear_psd_corr1 = higham_nearestPSD(corr_matrix_1)\n",
    "highamnear_psd_corr2 = higham_nearestPSD(corr_matrix_2)\n",
    "# Compare the results under atol=1e-5\n",
    "comparison_result_3_4 = np.isclose(highamnear_psd_corr1, highamnear_psd_corr2, atol=1e-5)\n",
    "print(\"Comparison result is:\\n\", comparison_result_3_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "092d62f7-769c-40df-9367-b29bd7794008",
   "metadata": {},
   "outputs": [],
   "source": [
    "###test 4###\n",
    "def chol_psd(a):\n",
    "    if not isinstance(a, pd.DataFrame):\n",
    "        a = pd.DataFrame(a)\n",
    "\n",
    "    m, n = a.shape\n",
    "    root = np.zeros((m, n))\n",
    "\n",
    "    for j in range(m):\n",
    "        s = 0.0\n",
    "        if j >= 0:\n",
    "            s = np.dot(root[j, :j], root[j, :j])\n",
    "        # Diagonal element\n",
    "        temp = a.iloc[j, j] - s\n",
    "        if -1e-8 <= temp <= 0:\n",
    "            temp = 0.0\n",
    "        root[j, j] = np.sqrt(max(temp, 0))\n",
    "\n",
    "        if root[j, j] == 0.0:\n",
    "            root[j, (j+1):n] = 0.0\n",
    "        else:\n",
    "            ir = 1.0 / root[j, j]\n",
    "            for i in range(j + 1, m):\n",
    "                s = np.dot(root[i, :j], root[j, :j])\n",
    "                root[i, j] = (a.iloc[i, j] - s) * ir\n",
    "    return root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ede633e-efe2-4b76-8464-1bf1090abcbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison result is:\n",
      " [[ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]]\n"
     ]
    }
   ],
   "source": [
    "#test 4.1\n",
    "# Load the correlation matrices from the provided files\n",
    "matrix_1 = pd.read_csv('/Users/queenieliu/FinTech545_Spring2024/testfiles/data/testout_3.1.csv').values\n",
    "matrix_2 = pd.read_csv('/Users/queenieliu/FinTech545_Spring2024/testfiles/data/testout_4.1.csv').values\n",
    "# Calculate the near PSD correlation matrices\n",
    "chol_psd1 = chol_psd(matrix_1)\n",
    "comparison_result_4 = np.isclose(chol_psd1, matrix_2, atol=1e-5)\n",
    "print(\"Comparison result is:\\n\", comparison_result_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "de27305c-eb31-48d8-9d00-4d66b4af4c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###test 5###\n",
    "def simulateNormal(N, df, mean=None, seed=1234, fixMethod=near_psd):\n",
    "    # Error Checking\n",
    "    m,n = df.shape\n",
    "    if n != m:\n",
    "        raise ValueError(f\"Covariance Matrix is not square ({n},{m})\")\n",
    "    # Initialize the output\n",
    "    out = np.zeros((N, n))\n",
    "    # Set mean\n",
    "    if mean is None:\n",
    "        mean = np.zeros(n)\n",
    "    else:\n",
    "        if len(mean) != n:\n",
    "            raise ValueError(f\"Mean ({len(mean)}) is not the size of cov ({n},{n})\")\n",
    "    # Set the seed to make sure the value is the same each time.\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    eigenvalues, eigenvectors = np.linalg.eig(df)\n",
    "    # If the covariance is not PS, try to fix it\n",
    "    if min(eigenvalues) < 0:\n",
    "        df = fixMethod (df)\n",
    "    # Take the root (cholesky factorization)\n",
    "    l = chol_psd(df)\n",
    "    # Generate random standard normals\n",
    "    rand_normals = np.random.normal(0.0, 1.0, size=(N, n))\n",
    "    # Apply the Cholesky root and plus the mean to the random normals\n",
    "    out = np.dot(rand_normals, l.T) + mean\n",
    "    \n",
    "    return out.T\n",
    "\n",
    "#Multivariate PCA Simulation\n",
    "def simulatePCA(N, df, mean=None, seed=1234, pctExp=1):\n",
    "    # Error Checking\n",
    "    m, n = df.shape\n",
    "    if n != m:\n",
    "        raise ValueError(f\"Covariance Matrix is not square ({n},{m})\")\n",
    "    # Initialize the output\n",
    "    out = np.zeros((N, n))\n",
    "    # Set mean\n",
    "    if mean is None:\n",
    "        mean = np.zeros(n)\n",
    "    else:\n",
    "        if len(mean) != n:\n",
    "            raise ValueError(f\"Mean ({len(mean)}) is not the size of cov ({n},{n})\")\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(df)\n",
    "    # Get the indices that would sort eigenvalues in descending order\n",
    "    indices = np.argsort(eigenvalues)[::-1]\n",
    "    # Sort eigenvalues\n",
    "    eigenvalues = eigenvalues[indices]\n",
    "    # Sort eigenvectors according to the same order\n",
    "    eigenvectors = eigenvectors[:, indices]\n",
    "    tv = np.sum(eigenvalues)\n",
    "    posv = np.where(eigenvalues >= 1e-8)[0]\n",
    "    if pctExp < 1:\n",
    "        nval = 0\n",
    "        pct = 0.0\n",
    "        # How many factors needed\n",
    "        for i in posv:\n",
    "            pct = pct + eigenvalues[i] / tv\n",
    "            nval = 1 + nval\n",
    "            if pct >= pctExp:\n",
    "                break\n",
    "     # If nval is less than the number of positive eigenvalues, truncate posv\n",
    "    if nval < len(posv):\n",
    "        posv = posv[:nval]\n",
    "    # Filter eigenvalues based on posv\n",
    "    eigenvalues = eigenvalues[posv]\n",
    "    eigenvectors = eigenvectors[:, posv]\n",
    "    \n",
    "    B = eigenvectors @ np.diag(np.sqrt(eigenvalues))\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    rand_normals = np.random.normal(0.0, 1.0, size=(N, len(posv)))\n",
    "    out = np.dot(rand_normals, B.T) + mean\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "13d92c06-6bee-4b6a-8d1e-1efcbd0e1abf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison result is:\n",
      " [[ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]]\n"
     ]
    }
   ],
   "source": [
    "#test 5.1\n",
    "df = pd.read_csv('/Users/queenieliu/FinTech545_Spring2024/testfiles/data/test5_1.csv')\n",
    "expected_res = pd.read_csv('/Users/queenieliu/FinTech545_Spring2024/testfiles/data/testout_5.1.csv')\n",
    "sim = simulateNormal(100000, df)\n",
    "result = np.cov(sim)\n",
    "comparison_result = np.isclose(expected_res, result, atol=1e-3)\n",
    "print(\"Comparison result is:\\n\", comparison_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "48ca8f79-5141-4bcf-87e4-3a38b9c98eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison result is:\n",
      " [[ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]]\n"
     ]
    }
   ],
   "source": [
    "#test 5.2\n",
    "df = pd.read_csv('/Users/queenieliu/FinTech545_Spring2024/testfiles/data/test5_2.csv')\n",
    "expected_res_5_2 = pd.read_csv('/Users/queenieliu/FinTech545_Spring2024/testfiles/data/testout_5.2.csv')\n",
    "sim = simulateNormal(100000, df)\n",
    "result_5_2 = np.cov(sim)\n",
    "comparison_result_5_2 = np.isclose(expected_res_5_2, result_5_2, atol=1e-3)\n",
    "print(\"Comparison result is:\\n\", comparison_result_5_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1bee53db-85cb-4048-b280-63de5f4c5e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison result is:\n",
      " [[ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]]\n"
     ]
    }
   ],
   "source": [
    "# testout_5.3\n",
    "df = pd.read_csv('/Users/queenieliu/FinTech545_Spring2024/testfiles/data/test5_3.csv')\n",
    "expected_res = pd.read_csv('/Users/queenieliu/FinTech545_Spring2024/testfiles/data/testout_5.3.csv')\n",
    "sim = simulateNormal(100000, df, fixMethod=near_psd)\n",
    "result = np.cov(sim)\n",
    "comparison_result_5_3 = np.isclose(expected_res, result, atol=1e-3)\n",
    "print(\"Comparison result is:\\n\", comparison_result_5_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9e8f0f8c-02da-4653-91c4-81fa5a8bcb1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison result is:\n",
      " [[ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]\n",
      " [ True  True  True  True  True]]\n"
     ]
    }
   ],
   "source": [
    "# testout_5.4\n",
    "df = pd.read_csv('/Users/queenieliu/FinTech545_Spring2024/testfiles/data/test5_3.csv')\n",
    "expected_res = pd.read_csv('/Users/queenieliu/FinTech545_Spring2024/testfiles/data/testout_5.4.csv')\n",
    "sim = simulateNormal(100000, df, fixMethod=higham_nearestPSD)\n",
    "result = np.cov(sim)\n",
    "comparison_result_5_4 = np.isclose(expected_res, result, atol=1e-3)\n",
    "print(\"Comparison result is:\\n\", comparison_result_5_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3feec547-3f4c-4a4f-8e45-bbf6038b73a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testout_5.5\n",
    "df = pd.read_csv('/Users/queenieliu/FinTech545_Spring2024/testfiles/data/test5_2.csv')\n",
    "expected_res = pd.read_csv('/Users/queenieliu/FinTech545_Spring2024/testfiles/data/testout_5.5.csv')\n",
    "sim = simulatePCA(100000, df, pctExp=0.99)\n",
    "result = np.cov(sim)\n",
    "comparison_result_5_5 = np.isclose(expected_res, result, atol=1e-3)\n",
    "print(\"Comparison result is:\\n\", comparison_result_5_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e5b81bc-1399-4b96-8ea6-c9184d4fb98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###test 6###\n",
    "def return_calculate(prices, method=\"DISCRETE\", date_column=\"Date\"):\n",
    "    if date_column not in prices.columns:\n",
    "        raise ValueError(f\"dateColumn: {date_column} not in DataFrame: {prices.columns.tolist()}\")\n",
    "    \n",
    "    # Extract the columns that are not the date column\n",
    "    vars = [col for col in prices.columns if col != date_column]\n",
    "    prices[vars] = prices[vars].apply(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "    p = prices[vars].values\n",
    "    n, m = p.shape\n",
    "    \n",
    "    p2 = np.empty((n-1, m))\n",
    "    p2 = p[1:] / p[:-1]\n",
    "\n",
    "    # Calculate the returns\n",
    "    if method.upper() == \"DISCRETE\":\n",
    "        # Discrete returns: (Price_t+1 / Price_t) - 1\n",
    "        p2 -= 1\n",
    "    elif method.upper() == \"LOG\":\n",
    "        # Log returns: log(Price_t+1 / Price_t)\n",
    "        p2 = np.log(p2)\n",
    "    else:\n",
    "        raise ValueError(f\"method: {method} must be in (\\\"LOG\\\", \\\"DISCRETE\\\")\")\n",
    "\n",
    "    # Create the output DataFrame\n",
    "    dates = prices[date_column].iloc[1:].reset_index(drop=True)\n",
    "    out = pd.DataFrame(data=p2, columns=vars)\n",
    "    out.insert(0, date_column, dates)\n",
    "    \n",
    "    return out\n",
    "#Load data\n",
    "prices = pd.read_csv('/Users/queenieliu/FinTech545_Spring2024/testfiles/data/test6.csv')\n",
    "discrete_returns = return_calculate(prices, method=\"DISCRETE\")\n",
    "log_returns = return_calculate(prices, method=\"LOG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "28b7d503-e637-4733-a4dc-bcbcb9eda5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All values in comparison_discrete: True\n"
     ]
    }
   ],
   "source": [
    "#test6_1\n",
    "expected_discrete_return = pd.read_csv('/Users/queenieliu/FinTech545_Spring2024/testfiles/data/test6_1.csv')\n",
    "computed_discrete_result = discrete_returns.drop(columns = ['Date']).to_numpy()\n",
    "expected_discrete_result = expected_discrete_return.drop(columns = ['Date']).to_numpy()\n",
    "comparison_discrete = np.isclose(expected_discrete_result, computed_discrete_result, atol=1e-5)\n",
    "all_true_discrete = np.all(comparison_discrete)\n",
    "print(\"All values in comparison_discrete:\", all_true_discrete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "767d4dc1-30f3-4020-8547-cf3a377b5e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All values in comparison_log: True\n"
     ]
    }
   ],
   "source": [
    "#test6_2\n",
    "expected_log_return = pd.read_csv('/Users/queenieliu/FinTech545_Spring2024/testfiles/data/test6_2.csv')\n",
    "computed_log_result = log_returns.drop(columns = ['Date']).to_numpy()\n",
    "expected_log_result = expected_log_return.drop(columns = ['Date']).to_numpy()\n",
    "comparison_log = np.isclose(expected_log_result, computed_log_result, atol=1e-5)\n",
    "all_true_log = np.all(comparison_log)\n",
    "print(\"All values in comparison_log:\", all_true_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76a6dc12-49e7-4a9f-bd40-cbc05271ba2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###test 7###\n",
    "def fit_normal(data):\n",
    "    # Fit the normal distribution to the data\n",
    "    mu, std = norm.fit(data)\n",
    "    return mu, std\n",
    "\n",
    "def fit_general_t(data):\n",
    "    # Fit the t distribution to the data\n",
    "    nu, mu, sigma = t.fit(data)\n",
    "    return mu, sigma, nu\n",
    "\n",
    "#Fit t distribution\n",
    "def fit_regression_t(df):\n",
    "    Y = df.iloc[:, -1]\n",
    "    X = df.iloc[:, :-1]\n",
    "    betas = MLE_t(X, Y)\n",
    "    X = sm.add_constant(X)\n",
    "    \n",
    "    # Get the residuals.\n",
    "    e = Y - np.dot(X, betas)\n",
    "\n",
    "    params = t.fit(e)\n",
    "    out = {\"mu\": [params[1]], \n",
    "           \"sigma\": [params[2]], \n",
    "           \"nu\": [params[0]]}\n",
    "    for i in range(len(betas)):\n",
    "        out[\"B\" + str(i)] = betas[i]\n",
    "    out = pd.DataFrame(out)\n",
    "    out.rename(columns={'B0': 'Alpha'}, inplace=True)\n",
    "    return out\n",
    "\n",
    "#The objective negative log-likelihood function (need to be minimized).\n",
    "def MLE_t(X, Y):\n",
    "    X = sm.add_constant(X)\n",
    "    def ll_t(params):\n",
    "        nu, sigma = params[:2]\n",
    "        beta_MLE_t = params[2:]\n",
    "        epsilon = Y - np.dot(X, beta_MLE_t)\n",
    "        # Calculate the log-likelihood\n",
    "        log_likelihood = np.sum(t.logpdf(epsilon, df=nu, loc=mu, scale=sigma))\n",
    "        return -log_likelihood\n",
    "    \n",
    "    beta = np.zeros(X.shape[1])\n",
    "    nu, mu, sigma = 1, 0, np.std(Y - np.dot(X, beta))\n",
    "    params = np.append([nu, sigma], beta)\n",
    "    bnds = ((0, None), (0, None), (None, None), (None, None), (None, None), (None, None))\n",
    "    \n",
    "    # Minimize the log-likelihood to get the beta\n",
    "    res = minimize(ll_t, params, bounds=bnds, options={'disp': True})\n",
    "    beta_MLE = res.x[2:]\n",
    "    return beta_MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "3527d1c7-3e79-49fc-9318-7071021656f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison result is:\n",
      " [[ True  True]]\n"
     ]
    }
   ],
   "source": [
    "#testout7_1\n",
    "df = pd.read_csv('/Users/queenieliu/FinTech545_Spring2024/testfiles/data/test7_1.csv')\n",
    "expected_res = pd.read_csv('/Users/queenieliu/FinTech545_Spring2024/testfiles/data/testout7_1.csv')\n",
    "result = fit_normal(df)\n",
    "comparison_result = np.isclose(expected_res, result, atol=1e-3)\n",
    "print(\"Comparison result is:\\n\", comparison_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "b71cb886-22ab-4a92-98bb-722044c16c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison result is:\n",
      " [[ True  True  True]]\n"
     ]
    }
   ],
   "source": [
    "#testout7_2\n",
    "df = pd.read_csv('/Users/queenieliu/FinTech545_Spring2024/testfiles/data/test7_2.csv')\n",
    "expected_res = pd.read_csv('/Users/queenieliu/FinTech545_Spring2024/testfiles/data/testout7_2.csv')\n",
    "result = fit_general_t(df)\n",
    "comparison_result = np.isclose(expected_res, result, atol=1e-3)\n",
    "print(\"Comparison result is:\\n\", comparison_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0d9d7fb-de18-4cbc-8f4b-dbcba2b77b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =            6     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  1.21632D+00    |proj g|=  2.91239D+01\n",
      "\n",
      "At iterate    1    f=  2.46768D-01    |proj g|=  3.49674D+01\n",
      "\n",
      "At iterate    2    f= -8.25857D+00    |proj g|=  6.36315D+01\n",
      "\n",
      "At iterate    3    f= -9.00656D+00    |proj g|=  2.01391D+01\n",
      "\n",
      "At iterate    4    f= -9.74769D+00    |proj g|=  1.89823D+01\n",
      "\n",
      "At iterate    5    f= -1.78442D+01    |proj g|=  2.00398D+02\n",
      "\n",
      "At iterate    6    f= -8.53086D+01    |proj g|=  2.05029D+02\n",
      "\n",
      "At iterate    7    f= -1.19412D+02    |proj g|=  1.47451D+02\n",
      "\n",
      "At iterate    8    f= -1.26128D+02    |proj g|=  1.47882D+02\n",
      "\n",
      "At iterate    9    f= -1.33571D+02    |proj g|=  1.20313D+02\n",
      "\n",
      "At iterate   10    f= -1.35629D+02    |proj g|=  1.38175D+02\n",
      "\n",
      "At iterate   11    f= -1.36856D+02    |proj g|=  7.11314D+01\n",
      "\n",
      "At iterate   12    f= -1.36866D+02    |proj g|=  4.68715D+01\n",
      "\n",
      "At iterate   13    f= -1.36932D+02    |proj g|=  3.40480D+00\n",
      "\n",
      "At iterate   14    f= -1.36933D+02    |proj g|=  3.43467D+00\n",
      "\n",
      "At iterate   15    f= -1.36934D+02    |proj g|=  4.03037D+00\n",
      "\n",
      "At iterate   16    f= -1.36940D+02    |proj g|=  1.54577D+01\n",
      "\n",
      "At iterate   17    f= -1.36952D+02    |proj g|=  3.08345D+01\n",
      "\n",
      "At iterate   18    f= -1.36986D+02    |proj g|=  5.66173D+01\n",
      "\n",
      "At iterate   19    f= -1.37066D+02    |proj g|=  9.20082D+01\n",
      "\n",
      "At iterate   20    f= -1.37230D+02    |proj g|=  1.28067D+02\n",
      "\n",
      "At iterate   21    f= -1.37467D+02    |proj g|=  1.26655D+02\n",
      "\n",
      "At iterate   22    f= -1.37516D+02    |proj g|=  1.32162D+02\n",
      "\n",
      "At iterate   23    f= -1.37696D+02    |proj g|=  5.35867D+01\n",
      "\n",
      "At iterate   24    f= -1.37729D+02    |proj g|=  7.00429D+00\n",
      "\n",
      "At iterate   25    f= -1.37732D+02    |proj g|=  6.77238D+00\n",
      "\n",
      "At iterate   26    f= -1.37735D+02    |proj g|=  7.72086D+00\n",
      "\n",
      "At iterate   27    f= -1.37744D+02    |proj g|=  9.33341D+00\n",
      "\n",
      "At iterate   28    f= -1.37760D+02    |proj g|=  9.68112D+00\n",
      "\n",
      "At iterate   29    f= -1.37783D+02    |proj g|=  6.60002D+00\n",
      "\n",
      "At iterate   30    f= -1.37794D+02    |proj g|=  5.51508D+00\n",
      "\n",
      "At iterate   31    f= -1.37805D+02    |proj g|=  4.45188D+00\n",
      "\n",
      "At iterate   32    f= -1.37808D+02    |proj g|=  8.47221D+00\n",
      "\n",
      "At iterate   33    f= -1.37812D+02    |proj g|=  1.37026D+01\n",
      "\n",
      "At iterate   34    f= -1.37815D+02    |proj g|=  1.32165D+01\n",
      "\n",
      "At iterate   35    f= -1.37819D+02    |proj g|=  7.13151D+00\n",
      "\n",
      "At iterate   36    f= -1.37820D+02    |proj g|=  2.38808D-01\n",
      "\n",
      "At iterate   37    f= -1.37820D+02    |proj g|=  9.29674D-02\n",
      "\n",
      "At iterate   38    f= -1.37820D+02    |proj g|=  9.29020D-02\n",
      "\n",
      "At iterate   39    f= -1.37820D+02    |proj g|=  1.16253D-01\n",
      "\n",
      "At iterate   40    f= -1.37820D+02    |proj g|=  2.50745D-01\n",
      "\n",
      "At iterate   41    f= -1.37820D+02    |proj g|=  4.95703D-01\n",
      "\n",
      "At iterate   42    f= -1.37820D+02    |proj g|=  8.53342D-01\n",
      "\n",
      "At iterate   43    f= -1.37820D+02    |proj g|=  1.44949D+00\n",
      "\n",
      "At iterate   44    f= -1.37821D+02    |proj g|=  2.11394D+00\n",
      "\n",
      "At iterate   45    f= -1.37824D+02    |proj g|=  6.19301D+00\n",
      "\n",
      "At iterate   46    f= -1.37828D+02    |proj g|=  6.03557D+00\n",
      "\n",
      "At iterate   47    f= -1.37840D+02    |proj g|=  2.60285D+01\n",
      "\n",
      "At iterate   48    f= -1.37854D+02    |proj g|=  7.85886D+00\n",
      "\n",
      "At iterate   49    f= -1.37870D+02    |proj g|=  3.09661D+00\n",
      "\n",
      "At iterate   50    f= -1.37878D+02    |proj g|=  3.30035D+00\n",
      "\n",
      "At iterate   51    f= -1.37879D+02    |proj g|=  3.46023D+00\n",
      "\n",
      "At iterate   52    f= -1.37881D+02    |proj g|=  3.05595D+00\n",
      "\n",
      "At iterate   53    f= -1.37881D+02    |proj g|=  8.15064D-01\n",
      "\n",
      "At iterate   54    f= -1.37881D+02    |proj g|=  1.85810D-01\n",
      "\n",
      "At iterate   55    f= -1.37881D+02    |proj g|=  5.18037D-01\n",
      "\n",
      "At iterate   56    f= -1.37881D+02    |proj g|=  1.19613D+00\n",
      "\n",
      "At iterate   57    f= -1.37881D+02    |proj g|=  7.90007D-01\n",
      "\n",
      "At iterate   58    f= -1.37881D+02    |proj g|=  1.73438D-01\n",
      "\n",
      "At iterate   59    f= -1.37881D+02    |proj g|=  2.50992D-02\n",
      "\n",
      "At iterate   60    f= -1.37881D+02    |proj g|=  2.56961D-02\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "    6     60     73     61     0     0   2.570D-02  -1.379D+02\n",
      "  F =  -137.88135418241896     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n",
      "Comparison result is:\n",
      " [[ True  True  True  True  True  True  True]]\n"
     ]
    }
   ],
   "source": [
    "#testout7_3\n",
    "df = pd.read_csv('/Users/queenieliu/FinTech545_Spring2024/testfiles/data/test7_3.csv')\n",
    "expected_res = pd.read_csv('/Users/queenieliu/FinTech545_Spring2024/testfiles/data/testout7_3.csv')\n",
    "result = fit_Tregression(df)\n",
    "comparison_result = np.isclose(expected_res, result, atol=1e-3)\n",
    "print(\"Comparison result is:\\n\", comparison_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42cef98e-bfc5-4989-ab4c-46f7e2216ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "###test 8###\n",
    "def fit_normal(data):\n",
    "    #Fit the normal distribution to the data\n",
    "    mu, std = norm.fit(data)\n",
    "    return mu, std\n",
    "def var_normal(data, alpha=0.05):\n",
    "    #Fit the data with normal distribution.\n",
    "    mu, std = fit_normal(data)\n",
    "    VaR = -norm.ppf(alpha, mu, std)\n",
    "    #Calculate the relative difference from the mean expected.\n",
    "    VaR_diff = VaR + mu\n",
    "    return pd.DataFrame({\"VaR Absolute\": [VaR], \n",
    "                         \"VaR Diff from Mean\": [VaR_diff]})\n",
    "    \n",
    "#ES for normal distribution\n",
    "def es_normal(data, alpha=0.05):\n",
    "    #Fit the data with normal distribution.\n",
    "    mu, std = fit_normal(data)\n",
    "\n",
    "    res = var_normal(data, alpha)\n",
    "    VaR = res.iloc[0, 0]\n",
    "    #Define the integrand function: x times the PDF of the distribution\n",
    "    def integrand(x, mu, std):\n",
    "        return x * norm.pdf(x, loc=mu, scale=std)\n",
    "    \n",
    "    ES, _ = quad(lambda x: integrand(x, mu, std), -np.inf, -VaR)\n",
    "    ES /= -alpha\n",
    "    #Calculate the relative difference from the mean expected.\n",
    "    ES_diff = ES + mu\n",
    "    return pd.DataFrame({\"ES Absolute\": [ES], \n",
    "                         \"ES Diff from Mean\": [ES_diff]})\n",
    "\n",
    "\n",
    "def fit_general_t(data):\n",
    "    #Fit the t distribution to the data\n",
    "    nu, mu, sigma = t.fit(data)\n",
    "    return mu, sigma, nu\n",
    "def var_t(data, alpha=0.05):\n",
    "    #Fit the data with t distribution.\n",
    "    mu, sigma, nu = fit_general_t(data)\n",
    "    VaR = -t.ppf(alpha, nu, mu, sigma)\n",
    "    #From the mean expected.\n",
    "    VaR_diff = VaR + mu\n",
    "    return pd.DataFrame({\"VaR Absolute\": [VaR], \n",
    "                         \"VaR Diff from Mean\": [VaR_diff]})\n",
    "    \n",
    "#VaR for t Distribution simulation \n",
    "def var_simulation(data, alpha=0.05, size=10000):\n",
    "    #Fit the data with t distribution.\n",
    "    mu, sigma, nu = fit_general_t(data)\n",
    "    #Generate given size random numbers from a t-distribution\n",
    "    random_numbers = t.rvs(df=nu, loc=mu, scale=sigma, size=size)\n",
    "    return var_t(random_numbers, alpha)\n",
    "\n",
    "#ES for t Distribution\n",
    "def es_t(data, alpha=0.05):\n",
    "    #Fit the data with normal distribution.\n",
    "    mu, sigma, nu = fit_general_t(data)\n",
    "    \n",
    "    res = var_t(data, alpha)\n",
    "    VaR = res.iloc[0, 0]\n",
    "    #Define the integrand function: x times the PDF of the distribution\n",
    "    def integrand(x, mu, sigma, nu):\n",
    "        return x * t.pdf(x, df=nu, loc=mu, scale=sigma)\n",
    "\n",
    "    ES, _ = quad(lambda x: integrand(x, mu, sigma, nu), -np.inf, -VaR)\n",
    "    ES /= -alpha\n",
    "    ES_diff = ES + mu\n",
    "    return pd.DataFrame({\"ES Absolute\": [ES], \n",
    "                         \"ES Diff from Mean\": [ES_diff]})\n",
    "\n",
    "#ES for simulation\n",
    "def es_simulation(data, alpha=0.05, size=10000):\n",
    "    #Fit the data with t distribution.\n",
    "    mu, sigma, nu = fit_general_t(data)\n",
    "    random_numbers = t.rvs(df=nu, loc=mu, scale=sigma, size=size)\n",
    "    return es_t(random_numbers, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d5177159-ec1e-4de1-a0a5-d13f07682ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison result is:\n",
      " [[ True  True]]\n"
     ]
    }
   ],
   "source": [
    "#testout8_1\n",
    "df = pd.read_csv('/Users/queenieliu/FinTech545_Spring2024/testfiles/data/test7_1.csv')\n",
    "expected_res = pd.read_csv('/Users/queenieliu/FinTech545_Spring2024/testfiles/data/testout8_1.csv')\n",
    "result = var_normal(df)\n",
    "comparison_result = np.isclose(expected_res, result, atol=1e-3)\n",
    "print(\"Comparison result is:\\n\", comparison_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2a410330-cc89-4831-8b96-e156d4b09714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison result is:\n",
      " [[ True  True]]\n"
     ]
    }
   ],
   "source": [
    "#testout8_2\n",
    "df = pd.read_csv('/Users/queenieliu/FinTech545_Spring2024/testfiles/data/test7_2.csv')\n",
    "expected_res = pd.read_csv('/Users/queenieliu/FinTech545_Spring2024/testfiles/data/testout8_2.csv')\n",
    "result = var_t(df)\n",
    "comparison_result = np.isclose(expected_res, result, atol=1e-3)\n",
    "print(\"Comparison result is:\\n\", comparison_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bba66118-8493-4fee-a4cc-c7f438f53155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison result is:\n",
      " [[ True  True]]\n"
     ]
    }
   ],
   "source": [
    "#testout8_3\n",
    "df = pd.read_csv('/Users/queenieliu/FinTech545_Spring2024/testfiles/data/test7_2.csv')\n",
    "expected_res = pd.read_csv('/Users/queenieliu/FinTech545_Spring2024/testfiles/data/testout8_3.csv')\n",
    "result = var_simulation(df, 0.05, 10000)\n",
    "comparison_result = np.isclose(expected_res, result, atol=1e-3)\n",
    "print(\"Comparison result is:\\n\", comparison_result)\n",
    "#the result is not always 'True', depends on the simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "99ab3264-f461-40be-9534-5d14f7cc6969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison result is:\n",
      " [[ True  True]]\n"
     ]
    }
   ],
   "source": [
    "#testout8_4\n",
    "df = pd.read_csv('/Users/queenieliu/FinTech545_Spring2024/testfiles/data/test7_1.csv')\n",
    "expected_res = pd.read_csv('/Users/queenieliu/FinTech545_Spring2024/testfiles/data/testout8_4.csv')\n",
    "# Calculate the ES at 5% quantile.\n",
    "result = es_normal(df, 0.05)\n",
    "comparison_result = np.isclose(expected_res, result, atol=1e-3)\n",
    "print(\"Comparison result is:\\n\", comparison_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e50a22f5-679e-48ba-aba0-5b3eee83cce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison result is:\n",
      " [[ True  True]]\n"
     ]
    }
   ],
   "source": [
    "#testout8_5\n",
    "df = pd.read_csv('/Users/queenieliu/FinTech545_Spring2024/testfiles/data/test7_2.csv')\n",
    "expected_res = pd.read_csv('/Users/queenieliu/FinTech545_Spring2024/testfiles/data/testout8_5.csv')\n",
    "# Calculate the ES at 5% quantile.\n",
    "result = es_t(df, 0.05)\n",
    "comparison_result = np.isclose(expected_res, result, atol=1e-3)\n",
    "print(\"Comparison result is:\\n\", comparison_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e2974918-f08f-45e7-a939-ef41a0d8ccb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison result is:\n",
      " [[ True  True]]\n"
     ]
    }
   ],
   "source": [
    "#testout8_6\n",
    "df = pd.read_csv('/Users/queenieliu/FinTech545_Spring2024/testfiles/data/test7_2.csv')\n",
    "expected_res = pd.read_csv('/Users/queenieliu/FinTech545_Spring2024/testfiles/data/testout8_6.csv')\n",
    "result = es_simulation(df, 0.05, 10000)\n",
    "comparison_result = np.isclose(expected_res, result, atol=1e-3)\n",
    "print(\"Comparison result is:\\n\", comparison_result)\n",
    "#the result is not always 'True', depends on the simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "104335af-0852-4e18-b6f9-cfaea3ccf32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm, t\n",
    "\n",
    "def return_calculate(prices, method='ARS', dateColumn='Date'):\n",
    "    # Exclude the date column from the calculations\n",
    "    tickers = [col for col in prices.columns if col != dateColumn]\n",
    "    prices = prices[tickers] # The dataframe is now with no date column\n",
    "    # Classical Brownian Motion\n",
    "    if method == 'CBM':\n",
    "        prices = prices.diff().dropna()\n",
    "    # Arithmetic Return System\n",
    "    elif method == 'ARS':\n",
    "        prices = (prices - prices.shift(1)) / prices.shift(1)\n",
    "        prices = prices.dropna()\n",
    "    # Geometric Brownian Motion\n",
    "    elif method == 'GBM':\n",
    "        prices = np.log(df).diff().dropna()\n",
    "    else:\n",
    "        raise ValueError(f\"method: {method} must be in (\\\"CBM\\\",\\\"ARS\\\",\\\"GBM\\\")\")\n",
    "    \n",
    "    return prices\n",
    "\n",
    "def simulatePCA(N, prices, mean=None, seed=1234, pctExp=1):\n",
    "    # Error Checking\n",
    "    m, n = prices.shape\n",
    "    if n != m:\n",
    "        raise ValueError(f\"Covariance Matrix is not square ({n},{m})\")\n",
    "    # Initialize output\n",
    "    out = np.zeros((N, n))\n",
    "    # Set mean\n",
    "    if mean is None:\n",
    "        mean = np.zeros(n)\n",
    "    else:\n",
    "        if len(mean) != n:\n",
    "            raise ValueError(f\"Mean ({len(mean)}) is not the size of cov ({n},{n})\")\n",
    "    \n",
    "    eigenvalues, eigenvectors = np.linalg.eig(prices)\n",
    "    \n",
    "    # Get the indices that would sort eigenvalues in descending order\n",
    "    indices = np.argsort(eigenvalues)[::-1]\n",
    "    # Sort eigenvalues\n",
    "    eigenvalues = eigenvalues[indices]\n",
    "    # Sort eigenvectors according to the same order\n",
    "    eigenvectors = eigenvectors[:, indices]\n",
    "    \n",
    "    tv = np.sum(eigenvalues)\n",
    "    posv = np.where(eigenvalues >= 1e-8)[0]\n",
    "    if pctExp <= 1:\n",
    "        nval = 0\n",
    "        pct = 0.0\n",
    "        # How many factors needed\n",
    "        for i in posv:\n",
    "            pct += eigenvalues[i] / tv\n",
    "            nval += 1\n",
    "            if pct >= pctExp:\n",
    "                break\n",
    "    \n",
    "     # If nval is less than the number of positive eigenvalues, truncate posv\n",
    "    if nval < len(posv):\n",
    "        posv = posv[:nval]\n",
    "        \n",
    "    # Filter eigenvalues based on posv\n",
    "    eigenvalues = eigenvalues[posv]\n",
    "    eigenvectors = eigenvectors[:, posv]\n",
    "    \n",
    "    B = eigenvectors @ np.diag(np.sqrt(eigenvalues))\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    rand_normals = np.random.normal(0.0, 1.0, size=(N, len(posv)))\n",
    "    out = np.dot(rand_normals, B.T) + mean\n",
    "    \n",
    "    return out.T\n",
    "\n",
    "def simulate_copula(portfolio, returns):\n",
    "    portfolio['CurrentValue'] = portfolio['Holding'] * portfolio['Starting Price']\n",
    "    models = {}\n",
    "    uniform = pd.DataFrame()\n",
    "    standard_normal = pd.DataFrame()\n",
    "    \n",
    "    for stock in portfolio[\"Stock\"]:\n",
    "        # If the distribution for the model is normal, fit the data with normal distribution\n",
    "        if portfolio.loc[portfolio['Stock'] == stock, 'Distribution'].iloc[0] == 'Normal':\n",
    "            models[stock] = norm.fit(returns[stock])\n",
    "            mu, sigma = norm.fit(returns[stock])\n",
    "            # Transform the observation vector into a uniform vector using CDF\n",
    "            uniform[stock] = norm.cdf(returns[stock], loc=mu, scale=sigma)\n",
    "            # Transform the uniform vector into a Standard Normal vector usig the normal quantile function\n",
    "            standard_normal[stock] = norm.ppf(uniform[stock])\n",
    "            \n",
    "        # If the distribution for the model is t, fit the data with normal t\n",
    "        elif portfolio.loc[portfolio['Stock'] == stock, 'Distribution'].iloc[0] == 'T':\n",
    "            models[stock] = t.fit(returns[stock])\n",
    "            nu, mu, sigma = t.fit(returns[stock])\n",
    "            # Transform the observation vector into a uniform vector using CDF\n",
    "            uniform[stock] = t.cdf(returns[stock], df=nu, loc=mu, scale=sigma)\n",
    "            # Transform the uniform vector into a Standard Normal vector usig the normal quantile function\n",
    "            standard_normal[stock] = norm.ppf(uniform[stock])\n",
    "        \n",
    "    # Calculate Spearman's correlation matrix\n",
    "    spearman_corr_matrix = standard_normal.corr(method='spearman')\n",
    "    \n",
    "    simulate_time = 10000\n",
    "    \n",
    "    # Use the PCA to simulate the multivariate normal\n",
    "    simulations = simulatePCA(simulate_time, spearman_corr_matrix)\n",
    "    simulations = pd.DataFrame(simulations.T, columns=[stock for stock in portfolio[\"Stock\"]])\n",
    "    # Transform the simulations into uniform variables using standard normal CDF\n",
    "    uni = norm.cdf(simulations)\n",
    "    uni = pd.DataFrame(uni, columns=[stock for stock in portfolio[\"Stock\"]])\n",
    "    simulatedReturns = pd.DataFrame()\n",
    "    # Transform the uniform variables into the desired data using quantile\n",
    "    for stock in portfolio[\"Stock\"]:\n",
    "        # If the distribution for the model is normal/t, use the quantile of the normal/t distribution\n",
    "        if portfolio.loc[portfolio['Stock'] == stock, 'Distribution'].iloc[0] == 'Normal':\n",
    "            mu, sigma = models[stock]\n",
    "            simulatedReturns[stock] = norm.ppf(uni[stock], loc=mu, scale=sigma)\n",
    "        elif portfolio.loc[portfolio['Stock'] == stock, 'Distribution'].iloc[0] == 'T':\n",
    "            nu, mu, sigma = models[stock]\n",
    "            simulatedReturns[stock] = t.ppf(uni[stock], df=nu, loc=mu, scale=sigma)\n",
    "    \n",
    "    simulatedValue = pd.DataFrame()\n",
    "    pnl = pd.DataFrame()\n",
    "    # Calculate the daily prices for each stock\n",
    "    for stock in portfolio[\"Stock\"]:\n",
    "        currentValue = portfolio.loc[portfolio['Stock'] == stock, 'CurrentValue'].iloc[0]\n",
    "        simulatedValue[stock] = currentValue * (1 + simulatedReturns[stock])\n",
    "        pnl[stock] = simulatedValue[stock] - currentValue\n",
    "        \n",
    "    risk = pd.DataFrame(columns = [\"Stock\", \"VaR95\", \"ES95\", \"VaR95_Pct\", \"ES95_Pct\"])\n",
    "    w = pd.DataFrame()\n",
    "\n",
    "    for stock in pnl.columns:\n",
    "        i = risk.shape[0]\n",
    "        risk.loc[i, \"Stock\"] = stock\n",
    "        risk.loc[i, \"VaR95\"] = -np.percentile(pnl[stock], 5)\n",
    "        risk.loc[i, \"VaR95_Pct\"] = risk.loc[i, \"VaR95\"] / portfolio.loc[portfolio['Stock'] == stock, 'CurrentValue'].iloc[0]\n",
    "        risk.loc[i, \"ES95\"] = -pnl[stock][pnl[stock] <= -risk.loc[i, \"VaR95\"]].mean()\n",
    "        risk.loc[i, \"ES95_Pct\"] = risk.loc[i, \"ES95\"] / portfolio.loc[portfolio['Stock'] == stock, 'CurrentValue'].iloc[0]\n",
    "        \n",
    "        # Determine the weights for the two stock\n",
    "        w.at['Weight', stock] = portfolio.loc[portfolio['Stock'] == stock, 'CurrentValue'].iloc[0] / portfolio['CurrentValue'].sum()\n",
    "        \n",
    "    # Calculate the total pnl\n",
    "    pnl['Total'] = 0\n",
    "    for stock in portfolio[\"Stock\"]:\n",
    "        pnl['Total'] += pnl[stock]\n",
    "    \n",
    "    i = risk.shape[0]\n",
    "    risk.loc[i, \"Stock\"] = 'Total'\n",
    "    risk.loc[i, \"VaR95\"] = -np.percentile(pnl['Total'], 5)\n",
    "    risk.loc[i, \"VaR95_Pct\"] = risk.loc[i, \"VaR95\"] / portfolio['CurrentValue'].sum()\n",
    "    risk.loc[i, \"ES95\"] = -pnl['Total'][pnl['Total'] <= -risk.loc[i, \"VaR95\"]].mean()\n",
    "    risk.loc[i, \"ES95_Pct\"] = risk.loc[i, \"ES95\"] / portfolio['CurrentValue'].sum()\n",
    "\n",
    "    return risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da6afd88-59aa-477e-a62d-7c913d2eec5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The expecetd outcome is:\n",
      "    Stock       VaR95        ES95  VaR95_Pct  ES95_Pct\n",
      "0      A   94.460376  118.289371   0.047230  0.059145\n",
      "1      B  107.880427  151.218174   0.035960  0.050406\n",
      "2  Total  152.565684  199.704532   0.030513  0.039941\n",
      "\n",
      "The calculated result is:\n",
      "    Stock       VaR95        ES95 VaR95_Pct  ES95_Pct\n",
      "0      A   93.131154  115.757315  0.046566  0.057879\n",
      "1      B  108.605932  153.976595  0.036202  0.051326\n",
      "2  Total   152.25331  202.826173  0.030451  0.040565\n"
     ]
    }
   ],
   "source": [
    "# testout_9.1\n",
    "df1 = pd.read_csv('/Users/queenieliu/FinTech545_Spring2024/testfiles/data/test9_1_portfolio.csv')\n",
    "df2 = pd.read_csv('/Users/queenieliu/FinTech545_Spring2024/testfiles/data/test9_1_returns.csv')\n",
    "expected_result = pd.read_csv('/Users/queenieliu/FinTech545_Spring2024/testfiles/data/testout9_1.csv')\n",
    "result = simulate_copula(df1, df2)\n",
    "print(\"The expecetd outcome is:\\n\", f'{expected_result}\\n')\n",
    "print(\"The calculated result is:\\n\", result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
